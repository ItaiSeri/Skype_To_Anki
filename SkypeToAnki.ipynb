{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fasttext\n",
    "import time\n",
    "from pathlib import Path\n",
    "from itertools import compress\n",
    "from googletrans import Translator\n",
    "from pycountry import languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator() #googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "#Local location of trained model for language detection. Explained in details here::\n",
    "# https://fasttext.cc/docs/en/language-identification.html\n",
    "fasttext_model_path=r\"C:\\Users\\Itai\\Anaconda3\\Lib\\site-packages\\fasttext\\lid.176.bin\" \n",
    "fasttext_model = fasttext.load_model(fasttext_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang='Chinese'\n",
    "src_langgoogle_code= 'zh-CN' #code list can be found here: https://cloud.google.com/translate/docs/languages\n",
    "start_date='2000-01-01'\n",
    "end_date='2099-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lang(character,src_lang):\n",
    "    lang_list=[]\n",
    "    predictions = fasttext_model.predict(character,k=5,threshold=0.7)[0]\n",
    "    for pred in predictions:\n",
    "        pred=pred.replace('__label__','')\n",
    "        if languages.get(alpha_2=pred) is not None:\n",
    "            lang_list.append(languages.get(alpha_2=pred).name)\n",
    "    return src_lang in lang_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitAndTrim(text,trim=True, split_del='\\n'): \n",
    "    if trim == True:\n",
    "        return text.replace(\" \",\"\").split(split_del)\n",
    "    else:\n",
    "        return text.split(split_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Skype_file_path=Path(r\"F:\\Dowloads\\8_itai.seri_export\\messages.json\")\n",
    "expath=Path(\"F:/Dowloads\")\n",
    "contact_list=['Anne Wu']\n",
    "#'Anne Wu','Wendy K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Skype2Anki(Skype_file_path,contact_list,src_lang,trim=True, split_del='\\n',start_date='2000-01-01',end_date='2099-01-01'):\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    #Read conversations column from Skype's Json file, then normalize. I.E Break up Series to df with meaningful columns\n",
    "    normalized=pd.json_normalize(pd.read_json(Skype_file_path,encoding='utf-8').conversations)\n",
    "    \n",
    "    #Convert the MessageList of every contact to df, then concat all of them into one df \n",
    "    df=pd.concat(normalized.MessageList[normalized.displayName.isin(contact_list)].apply(lambda x: pd.DataFrame(x)).tolist())\n",
    "       \n",
    "    #Convert originalarrivaltime to datetime & filter df by datetime\n",
    "    df.originalarrivaltime=pd.to_datetime(df.originalarrivaltime)\n",
    "    df=df[(df['originalarrivaltime'] > start_date) & (df['originalarrivaltime'] < end_date)]\n",
    "    \n",
    "    #delete white spaces needs to be only in asian languages\n",
    "    df['content_trim_split']= df['content'].apply(lambda x: SplitAndTrim(x,trim, split_del))\n",
    "     #apply the function for every word\n",
    "    df['is_src_lang']= df['content_trim_split'].apply(lambda x: [check_lang(string,src_lang) for string in x])\n",
    "    #Delete lists where text is not in the source language\n",
    "    df['clean_content']= df[['content_trim_split','is_src_lang']].apply(tuple,axis=1).apply(lambda x: list(compress(x[0],x[1])))\n",
    "    \n",
    "    # Leave only rows with non empty list. \"not x\": True for empty list, False for not empty list\n",
    "    # Split rows with multiple lists to multiple rows (explode)\n",
    "    fltrd_df=df.clean_content[df['clean_content'].apply(lambda x: not x) ==False].explode().reset_index(drop=True) \n",
    "    \n",
    "    #Transle every row to destination language\n",
    "    Translation=fltrd_df.apply(lambda x: translator.translate(x).text)\n",
    "    #Get pronunciation of source language for every row\n",
    "    Pinyin=fltrd_df.apply(lambda x: translator.translate(x,dest=src_langgoogle_code).pronunciation)\n",
    "    Anki_CSV=pd.concat([fltrd_df,Translation,Pinyin],axis=1)\n",
    "    Anki_CSV.columns=['Source','Translation','Pinyin']\n",
    "    file=expath/'Anki_CSV.txt'\n",
    "    file.unlink(missing_ok=True) #Delete file if exists\n",
    "    Anki_CSV.to_csv(file, index=None, mode='a')\n",
    "    end = time.time()\n",
    "    runtime= end - start\n",
    "    print('CSV file created after '+ str(runtime) + ' seconds.\\nLocation: ' +str(expath/'Anki_CSV.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created after 21.51619839668274 seconds.\n",
      "Location: F:\\Dowloads\\Anki_CSV.txt\n"
     ]
    }
   ],
   "source": [
    "Skype2Anki(Skype_file_path,contact_list,src_lang,trim=True, split_del='\\n',start_date='2000-01-01',end_date='2099-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
